{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import fasttext\n",
    "import json\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PATH_TO_DATASETS = \"../datasets\"\n",
    "PATH_TO_POLEMO_CONLL = \"../datasets/polemo/dataset_conll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "files = {\n",
    "    \"train\": os.path.join(PATH_TO_POLEMO_CONLL,\"all.sentence.train_processed.csv\"),\n",
    "    \"dev\": os.path.join(PATH_TO_POLEMO_CONLL,\"all.sentence.dev_processed.csv\"),\n",
    "    \"test\": os.path.join(PATH_TO_POLEMO_CONLL,\"all.sentence.test_processed.csv\"),\n",
    "    \"annotation\": os.path.join(PATH_TO_DATASETS, \"sentiment_data\", \"political_tweets_annotations.csv\")\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "with open(os.path.join(\"..\", \"datasets\", \"emojis.json\"), encoding=\"utf-8\") as f:\n",
    "    emoji_mapping = json.load(f)\n",
    "\n",
    "emoji_mapping_items = emoji_mapping.items()\n",
    "def emoji2text_tweet(tweet: str) -> str:\n",
    "    text = tweet\n",
    "    for emoji, emoji_text in emoji_mapping_items:\n",
    "        text = text.replace(emoji, f\"<{emoji_text}>\")\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def remove_quotes_from_saved_file(txt_path: str):\n",
    "    text = \"\"\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line[0] == \"\\\"\" and line[-2] == \"\\\"\":\n",
    "                line = line[1:]\n",
    "                line = line[:-2] + \"\\n\"\n",
    "            text += line\n",
    "\n",
    "    os.remove(txt_path)\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "data_for_fasttext = {}\n",
    "for dataset, file_path in files.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[['label','text']]\n",
    "    df['label'] = \"__label__\" +  df['label']\n",
    "    df['text'] = df['text'].apply(emoji2text_tweet)\n",
    "    df['text'] = df['text'].apply(lambda string: string.lower())\n",
    "    df['text'] = df['text'].apply(lambda string: string.replace(\"#\",\"\"))\n",
    "    df['row'] = df['label'] + \" \" + df['text']\n",
    "    path = os.path.join(PATH_TO_DATASETS, \"sentiment_data\", f\"{dataset}_data.txt\")\n",
    "    df['row'].to_csv(path, index=False, header=False)\n",
    "    remove_quotes_from_saved_file(path)\n",
    "    data_for_fasttext[dataset] = {}\n",
    "    data_for_fasttext[dataset][\"labels\"] = list(df['label'].values)\n",
    "    data_for_fasttext[dataset][\"texts\"] = list(df['text'].values)\n",
    "    data_for_fasttext[dataset][\"dataframe\"] = df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "tweets_data = data_for_fasttext['annotation']\n",
    "texts_train_val, texts_test, labels_train_val, labels_test = train_test_split(tweets_data['texts'], tweets_data['labels'], test_size=0.1, random_state=42)\n",
    "texts_train, texts_val, labels_train, labels_val = train_test_split(texts_train_val, labels_train_val, test_size=1/9, random_state=42)\n",
    "\n",
    "train_polemo = data_for_fasttext['train']['dataframe'][[\"text\", \"label\"]]\n",
    "val_polemo = data_for_fasttext['dev']['dataframe'][[\"text\", \"label\"]]\n",
    "test_polemo = data_for_fasttext['test']['dataframe'][[\"text\", \"label\"]]\n",
    "\n",
    "train_tweets = pd.DataFrame(data={\"text\": texts_train, \"label\": labels_train})\n",
    "val_tweets = pd.DataFrame(data={\"text\": texts_val, \"label\": labels_val})\n",
    "test_tweets = pd.DataFrame(data={\"text\": texts_test, \"label\": labels_test})\n",
    "\n",
    "train = train_polemo.append(train_tweets)\n",
    "val = val_polemo.append(val_tweets)\n",
    "test = test_polemo.append(test_tweets)\n",
    "\n",
    "train['row'] = train['label'] + \" \" + train['text']\n",
    "val['row'] = val['label'] + \" \" + val['text']\n",
    "test['row'] = test['label'] + \" \" + test['text']\n",
    "\n",
    "train['row'].to_csv(os.path.join(PATH_TO_DATASETS, \"sentiment_data\", f\"full_train_data.txt\"), index=False, header=False)\n",
    "val['row'].to_csv(os.path.join(PATH_TO_DATASETS, \"sentiment_data\", f\"full_val_data.txt\"), index=False, header=False)\n",
    "test['row'].to_csv(os.path.join(PATH_TO_DATASETS, \"sentiment_data\", f\"full_test_data.txt\"), index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier only on PolEmo training data...\n",
      "F1-score for all tweets: 0.29466757589906445\n",
      "F1-score for test set of PolEmo: 0.5874823306117167\n",
      "\n",
      "Training classifier on PolEmo training data and 80% of political tweets\n",
      "F1-score for test set of political tweets: 0.3812850008377031\n",
      "F1-score for test set of PolEmo: 0.5610325791091721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training classifier only on PolEmo training data...\")\n",
    "model =fasttext.train_supervised(input=os.path.join(PATH_TO_DATASETS, \"sentiment_data\", \"train_data.txt\"), wordNgrams=1, neg=5,dim=300, lr=0.005, epoch=500, loss=\"ns\", verbose=1, label_prefix='__label__')\n",
    "test_results = model.predict(data_for_fasttext[\"test\"][\"texts\"])\n",
    "annotation_results = model.predict(data_for_fasttext[\"annotation\"][\"texts\"])\n",
    "\n",
    "print(f\"F1-score for all tweets: {f1_score(annotation_results[0],data_for_fasttext['annotation']['labels'],average='macro')}\")\n",
    "print(f\"F1-score for test set of PolEmo: {f1_score(test_results[0],data_for_fasttext['test']['labels'],average='macro')}\")\n",
    "print()\n",
    "\n",
    "print(\"Training classifier on PolEmo training data and 80% of political tweets\")\n",
    "model =fasttext.train_supervised(input=os.path.join(PATH_TO_DATASETS, \"sentiment_data\", \"full_train_data.txt\"), wordNgrams=1, neg=5,dim=300, lr=0.005, epoch=500, loss=\"ns\", verbose=1, label_prefix='__label__')\n",
    "test_results = model.predict(list(test_polemo[\"text\"].values))\n",
    "annotation_results = model.predict(list(test_tweets['text'].values))\n",
    "\n",
    "print(f\"F1-score for test set of political tweets: {f1_score(annotation_results[0],list(test_tweets['label'].values),average='macro')}\")\n",
    "print(f\"F1-score for test set of PolEmo: {f1_score(test_results[0],list(test_polemo['label'].values),average='macro')}\")\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5623942811063802\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5629298826329951\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.562571859518963\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.3500344840528849\n",
      "F1-score for dev set: 0.5629021398226923\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.36152677445247106\n",
      "F1-score for dev set: 0.5582780004991592\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.3601970461953892\n",
      "F1-score for dev set: 0.5592922691828756\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.367239299716516\n",
      "F1-score for dev set: 0.5564198703181898\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.367239299716516\n",
      "F1-score for dev set: 0.5560494890712804\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5706073642332963\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5704664458637649\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5700849443316581\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.57068949669332\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.41809287758654845\n",
      "F1-score for dev set: 0.5964858413727783\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.41809287758654845\n",
      "F1-score for dev set: 0.5961174314759348\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.41809287758654845\n",
      "F1-score for dev set: 0.5962649553694438\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.41809287758654845\n",
      "F1-score for dev set: 0.5965323319189448\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6031817829531319\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6022700759160577\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.601864468810706\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6015863097213002\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.605732347798267\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6054246922025561\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6056461452059755\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6041908521269723\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.39900082638419354\n",
      "F1-score for dev set: 0.5902343640042036\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.39900082638419354\n",
      "F1-score for dev set: 0.5899936012340775\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.39900082638419354\n",
      "F1-score for dev set: 0.5897528640697065\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.39900082638419354\n",
      "F1-score for dev set: 0.5898151012624974\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4330852287247636\n",
      "F1-score for dev set: 0.5964765228893903\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43242796360266245\n",
      "F1-score for dev set: 0.5963029705464398\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43242796360266245\n",
      "F1-score for dev set: 0.5950023738027166\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43242796360266245\n",
      "F1-score for dev set: 0.5942141939826172\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.45773524720893144\n",
      "F1-score for dev set: 0.595838509699971\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.45685341299376386\n",
      "F1-score for dev set: 0.5959876525220141\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.45685341299376386\n",
      "F1-score for dev set: 0.5963027592912202\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.45685341299376386\n",
      "F1-score for dev set: 0.5961264799049685\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4287878787878788\n",
      "F1-score for dev set: 0.578241469359867\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.4287878787878788\n",
      "F1-score for dev set: 0.5780949693429496\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.4287878787878788\n",
      "F1-score for dev set: 0.5780318706504539\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.4287878787878788\n",
      "F1-score for dev set: 0.5781989423181655\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.58565814271643\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.5846522159345713\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.5849852841231988\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.5859802479593381\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5883855813828512\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5885487698767945\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5886762355254658\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5886762355254658\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5717001098326694\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.572027866196775\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5718699674227063\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5718699674227063\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4989115474077759\n",
      "F1-score for dev set: 0.578612027993442\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.4885352339814949\n",
      "F1-score for dev set: 0.5793203197874702\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.4627725035161744\n",
      "F1-score for dev set: 0.5767334745329324\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.4885352339814949\n",
      "F1-score for dev set: 0.5750089062371991\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.44657598867835524\n",
      "F1-score for dev set: 0.5799423032404591\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.44657598867835524\n",
      "F1-score for dev set: 0.5807364590481098\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.44657598867835524\n",
      "F1-score for dev set: 0.5807364590481098\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.44657598867835524\n",
      "F1-score for dev set: 0.5807364590481098\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5625205958490247\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5617300653618835\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5626973463916634\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5623865660464368\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.36612220873339957\n",
      "F1-score for dev set: 0.5564832202428006\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.36612220873339957\n",
      "F1-score for dev set: 0.5578504995255673\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.367239299716516\n",
      "F1-score for dev set: 0.5563937762031202\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.3611425339366516\n",
      "F1-score for dev set: 0.5551823336884754\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5705791619755132\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5694953991109653\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5708618017026222\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5695170101135963\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5960740521596544\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5960522855118234\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5958891375307778\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5959014017503925\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6024626999103988\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6024241284092978\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6008479437826201\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6006420705828094\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6038631575992107\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6047068496903283\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6043803336380296\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6043846940144177\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.5876384828014878\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.5887840972994121\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.5881732757452676\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.5880546767550845\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4330852287247636\n",
      "F1-score for dev set: 0.5962680358700545\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43242796360266245\n",
      "F1-score for dev set: 0.5968775810451735\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43242796360266245\n",
      "F1-score for dev set: 0.5951143331641728\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43242796360266245\n",
      "F1-score for dev set: 0.5943461994806127\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.45773524720893144\n",
      "F1-score for dev set: 0.5964579763289016\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.45773524720893144\n",
      "F1-score for dev set: 0.5961063100557479\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.45773524720893144\n",
      "F1-score for dev set: 0.596329579869589\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.45773524720893144\n",
      "F1-score for dev set: 0.5963056140151539\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.42163633467981293\n",
      "F1-score for dev set: 0.5767430521840395\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.42163633467981293\n",
      "F1-score for dev set: 0.5767313243560706\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.42163633467981293\n",
      "F1-score for dev set: 0.5765753511024818\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.42163633467981293\n",
      "F1-score for dev set: 0.5765931618705866\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.5855133146722856\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.5845263915117236\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.5851894549453599\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.585169571399017\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5878358317484796\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5871059184116542\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.587897041775314\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5881741909456453\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5698290272947197\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5696321884357409\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5698290272947197\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5698033238253848\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4989115474077759\n",
      "F1-score for dev set: 0.5781318888545635\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.4885352339814949\n",
      "F1-score for dev set: 0.580648497585905\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.4885352339814949\n",
      "F1-score for dev set: 0.5782755314640016\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.4885352339814949\n",
      "F1-score for dev set: 0.5773602750116107\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4393732492997199\n",
      "F1-score for dev set: 0.5788925140297055\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.4393732492997199\n",
      "F1-score for dev set: 0.5795021232622986\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.44657598867835524\n",
      "F1-score for dev set: 0.5786816349500974\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.44657598867835524\n",
      "F1-score for dev set: 0.5796255100354513\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5632997887206401\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.3500344840528849\n",
      "F1-score for dev set: 0.5627443738750487\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.3500344840528849\n",
      "F1-score for dev set: 0.5634742391696247\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.3500344840528849\n",
      "F1-score for dev set: 0.5629397978184261\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.36612220873339957\n",
      "F1-score for dev set: 0.557038219265292\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.3600246477577898\n",
      "F1-score for dev set: 0.5581052282131655\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.37326506587625674\n",
      "F1-score for dev set: 0.5568494278760834\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.37326506587625674\n",
      "F1-score for dev set: 0.5552685793608568\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5701158764554315\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5694119001074792\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5693101927126345\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5700904054979502\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5946319975901299\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5943110117810346\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5948872064014958\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5943293136436751\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6026093285004853\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6018275226901302\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6013123296488856\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6007268472771755\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6043218635195131\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6044032434171132\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6042658336779875\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6048047717004964\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.5880047168330594\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.588038206554259\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.5881203378409153\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.39900082638419354\n",
      "F1-score for dev set: 0.5876451344999448\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4330852287247636\n",
      "F1-score for dev set: 0.5960209072677074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dims = []\n",
    "ngrams = []\n",
    "losses = []\n",
    "negs = []\n",
    "tweets_f1_scores = []\n",
    "dev_f1_scores = []\n",
    "for dim in [300, 500]:\n",
    "    for ngram in [1,2,3,4,5]:\n",
    "        for method in [\"hs\", \"ns\", \"softmax\"]:\n",
    "            for neg in [5,10,15,20]:\n",
    "                model =fasttext.train_supervised(input=os.path.join(PATH_TO_DATASETS, \"sentiment_data\", \"full_train_data.txt\"), wordNgrams=ngram, neg=neg,dim=dim, lr=0.005, epoch=500, loss=method, verbose=1, label_prefix='__label__')\n",
    "                dev_results = model.predict(list(val_polemo[\"text\"].values))\n",
    "                annotation_results = model.predict(list(val_tweets[\"text\"].values))\n",
    "                dims.append(dim)\n",
    "                ngrams.append(ngram)\n",
    "                losses.append(method)\n",
    "                negs.append(neg)\n",
    "                tweets_f1_scores.append(f1_score(annotation_results[0],list(val_tweets['label'].values),average='macro'))\n",
    "                dev_f1_scores.append(f1_score(dev_results[0],list(val_polemo['label'].values),average='macro'))\n",
    "                print(f\"Loss method - {method}\")\n",
    "                print(f\"Dim - {dim}\")\n",
    "                print(f\"Ngram - {ngram}\")\n",
    "                print(f\"Negative samples - {neg}\")\n",
    "                print(f\"F1-score for all tweets: {tweets_f1_scores[-1]}\")\n",
    "                print(f\"F1-score for dev set: {dev_f1_scores[-1]}\")\n",
    "                print()\n",
    "\n",
    "results = pd.DataFrame(data={\"dim\": dims,\n",
    "                             \"ngram\" : ngrams,\n",
    "                             \"loss\": losses,\n",
    "                             \"neg\": negs,\n",
    "                             \"tweets_f1_score\": tweets_f1_scores,\n",
    "                             \"dev_f1_score\": dev_f1_scores})\n",
    "\n",
    "results.to_csv(os.path.join(\"..\",\"reports\",\"sentiment_classification_results.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate - 0.001\n",
      "Epochs - 100\n",
      "F1-score for all tweets: 0.07723577235772357\n",
      "F1-score for dev set: 0.13483288855000636\n",
      "\n",
      "Learning rate - 0.001\n",
      "Epochs - 250\n",
      "F1-score for all tweets: 0.28369486793399834\n",
      "F1-score for dev set: 0.28733619777780717\n",
      "\n",
      "Learning rate - 0.001\n",
      "Epochs - 500\n",
      "F1-score for all tweets: 0.3450963718820861\n",
      "F1-score for dev set: 0.4875128174787702\n",
      "\n",
      "Learning rate - 0.001\n",
      "Epochs - 1000\n",
      "F1-score for all tweets: 0.44096459096459095\n",
      "F1-score for dev set: 0.5607791230695027\n",
      "\n",
      "Learning rate - 0.005\n",
      "Epochs - 100\n",
      "F1-score for all tweets: 0.3450963718820861\n",
      "F1-score for dev set: 0.4872309231673328\n",
      "\n",
      "Learning rate - 0.005\n",
      "Epochs - 250\n",
      "F1-score for all tweets: 0.4589617898441427\n",
      "F1-score for dev set: 0.5740606491264695\n",
      "\n",
      "Learning rate - 0.005\n",
      "Epochs - 500\n",
      "F1-score for all tweets: 0.4989115474077759\n",
      "F1-score for dev set: 0.5780589677968582\n",
      "\n",
      "Learning rate - 0.005\n",
      "Epochs - 1000\n",
      "F1-score for all tweets: 0.4441432764630343\n",
      "F1-score for dev set: 0.5730638765390956\n",
      "\n",
      "Learning rate - 0.0001\n",
      "Epochs - 100\n",
      "F1-score for all tweets: 0.08022774327122154\n",
      "F1-score for dev set: 0.12492594192404008\n",
      "\n",
      "Learning rate - 0.0001\n",
      "Epochs - 250\n",
      "F1-score for all tweets: 0.11953551912568307\n",
      "F1-score for dev set: 0.1347864768683274\n",
      "\n",
      "Learning rate - 0.0001\n",
      "Epochs - 500\n",
      "F1-score for all tweets: 0.07723577235772357\n",
      "F1-score for dev set: 0.13483288855000636\n",
      "\n",
      "Learning rate - 0.0001\n",
      "Epochs - 1000\n",
      "F1-score for all tweets: 0.07377049180327869\n",
      "F1-score for dev set: 0.13483288855000636\n",
      "\n",
      "Learning rate - 0.0005\n",
      "Epochs - 100\n",
      "F1-score for all tweets: 0.07723577235772357\n",
      "F1-score for dev set: 0.13483288855000636\n",
      "\n",
      "Learning rate - 0.0005\n",
      "Epochs - 250\n",
      "F1-score for all tweets: 0.07377049180327869\n",
      "F1-score for dev set: 0.135181151247235\n",
      "\n",
      "Learning rate - 0.0005\n",
      "Epochs - 500\n",
      "F1-score for all tweets: 0.2913510101010101\n",
      "F1-score for dev set: 0.2865765515902747\n",
      "\n",
      "Learning rate - 0.0005\n",
      "Epochs - 1000\n",
      "F1-score for all tweets: 0.3450963718820861\n",
      "F1-score for dev set: 0.48687333663868504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrs = []\n",
    "epochs = []\n",
    "tweets_f1_scores = []\n",
    "dev_f1_scores = []\n",
    "for lr in [0.001, 0.005, 0.0001, 0.0005]:\n",
    "    for epoch in [100,250,500,1000]:\n",
    "        model =fasttext.train_supervised(input=os.path.join(PATH_TO_DATASETS, \"sentiment_data\", \"full_train_data.txt\"), wordNgrams=5, neg=5,dim=300, lr=lr, epoch=epoch, loss=\"ns\", verbose=1, label_prefix='__label__')\n",
    "        dev_results = model.predict(list(val_polemo[\"text\"].values))\n",
    "        annotation_results = model.predict(list(val_tweets[\"text\"].values))\n",
    "        lrs.append(lr)\n",
    "        epochs.append(epoch)\n",
    "        tweets_f1_scores.append(f1_score(annotation_results[0],list(val_tweets['label'].values),average='macro'))\n",
    "        dev_f1_scores.append(f1_score(dev_results[0],list(val_polemo['label'].values),average='macro'))\n",
    "        print(f\"Learning rate - {lr}\")\n",
    "        print(f\"Epochs - {epoch}\")\n",
    "        print(f\"F1-score for all tweets: {tweets_f1_scores[-1]}\")\n",
    "        print(f\"F1-score for dev set: {dev_f1_scores[-1]}\")\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "lr_epoch_results = pd.DataFrame(data={\"lr\": lrs,\n",
    "                             \"epoch\" : epochs,\n",
    "                             \"tweets_f1_score\": tweets_f1_scores,\n",
    "                             \"dev_f1_score\": dev_f1_scores})\n",
    "\n",
    "lr_epoch_results.to_csv(os.path.join(\"..\",\"reports\",\"sentiment_classification_lr_epoch_results.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=os.path.join(PATH_TO_DATASETS, \"sentiment_data\", \"full_train_data.txt\"), wordNgrams=5, neg=5,dim=300, lr=0.005, epoch=500, loss=\"ns\", verbose=1, label_prefix='__label__')\n",
    "test_results = model.predict(list(test_polemo[\"text\"].values))\n",
    "annotation_results = model.predict(list(test_tweets[\"text\"].values))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for all tweets: 0.42461047925843143\n",
      "F1-score for dev set: 0.5924934125716439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1-score for all tweets: {f1_score(annotation_results[0],list(test_tweets['label'].values),average='macro')}\")\n",
    "print(f\"F1-score for dev set: {f1_score(test_results[0],list(test_polemo['label'].values),average='macro')}\")\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for political tweets\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "__label__ambiguous       0.00      0.00      0.00         5\n",
      " __label__negative       0.52      0.62      0.57        24\n",
      "  __label__neutral       0.49      0.69      0.57        26\n",
      " __label__positive       0.70      0.47      0.56        49\n",
      "\n",
      "          accuracy                           0.54       104\n",
      "         macro avg       0.43      0.45      0.42       104\n",
      "      weighted avg       0.57      0.54      0.54       104\n",
      "\n",
      "Classification report for polemo data\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "__label__ambiguous       0.48      0.23      0.31       681\n",
      " __label__negative       0.63      0.84      0.72      2123\n",
      "  __label__neutral       0.69      0.61      0.65      1419\n",
      " __label__positive       0.75      0.66      0.70      1522\n",
      "\n",
      "          accuracy                           0.66      5745\n",
      "         macro avg       0.64      0.58      0.59      5745\n",
      "      weighted avg       0.66      0.66      0.65      5745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report for political tweets\")\n",
    "print(classification_report(list(test_tweets['label'].values), annotation_results[0]))\n",
    "\n",
    "print(\"Classification report for polemo data\")\n",
    "print(classification_report(list(test_polemo['label'].values), test_results[0]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets which were incorrrectly predicted:\n",
      "Tweet text: dzisiaj spotkanie zgip z parlamentarzystami z woj. l. dobra frekwencja. nikt z posw nie broni krytykowanych przez samorzd projektw.\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.14805719256401062\n",
      "\n",
      "Tweet text: projekt gotowy! <do z palcem wskazujcym w prawo> dotrzymuj sowa. 18 maja 2019 r. podczas spotkania z emerytowanymi grnikami w wabrzychu obiecaam przygotowa nowelizacj ustawy z dnia 17 grudnia 1998 r. o emeryturach i rentach z funduszu ubezpiecze spoecznych. sejmrp nowelizacja emeryci grnicy\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.44553956389427185\n",
      "\n",
      "Tweet text: premier jest ju w pszczyna dotrzymujemysowa\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.11597072333097458\n",
      "\n",
      "Tweet text: to jest skandal!\n",
      "\t True label: __label__negative, predicted label: __label__ambiguous, probability: 0.0015587612288072705\n",
      "\n",
      "Tweet text: - nie zejdziemy ze swej drogi, dalej bdziemy reformowa wymiar sprawiedliwoci, bo tego potrzebuje rzeczpospolita i jej obywatele, nie ugniemy si pod naciskami ani wewntrznymi, ani zewntrznymi  napisa j.kaczyski do uczestnikw konwencji sprawiedliwapolska\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.6859594583511353\n",
      "\n",
      "Tweet text: po pierwsze, jestem na posiedzeniu komisji ustawodawczej, ktra pracuje nad ustaw o komisji ds. pedofilii. po drugie, nie mog uczestniczy w pracach organu, ktrego wikszo skadu zostaa wybrana z naruszeniem konstytucji.\n",
      "\t True label: __label__neutral, predicted label: __label__negative, probability: 0.3775506913661957\n",
      "\n",
      "Tweet text: dzi wybory do izb rolniczych idcie gosowa <przycisk z krzyykiem> to wane\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.3276783227920532\n",
      "\n",
      "Tweet text: prezydent : ja nie mam adnego problemu z tym, e czy s bardzo majtnymi ludmi. ja mam problem z tym, e przepisuj majtki na ony i probuj to ukry. arenaprezydencka trzaskowski2020\n",
      "\t True label: __label__negative, predicted label: __label__positive, probability: 0.13297423720359802\n",
      "\n",
      "Tweet text: kocham <serce>\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.1968362182378769\n",
      "\n",
      "Tweet text: wyruszamy z twojsztab na spotkania z mieszkacami mazowsza i woj. podlaskiego porozmawiajmy\n",
      "\t True label: __label__neutral, predicted label: __label__positive, probability: 0.22271016240119934\n",
      "\n",
      "Tweet text: z pozdrowieniami dla pewnego ksidza...\n",
      "\t True label: __label__negative, predicted label: __label__positive, probability: 0.14805719256401062\n",
      "\n",
      "Tweet text: grzegorz braun: naprzd polsko! zarzdzajmy narodow werw w miejsce obecnego zarzdzania strachem! - youtube\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.4843900501728058\n",
      "\n",
      "Tweet text: prezes jarosaw kaczyski , rada polityczna prawa i sprawiedliwoci przez aklamacj popara kandydatur andrzeja dudy na prezydenta rp.\n",
      "\t True label: __label__neutral, predicted label: __label__positive, probability: 0.29422497749328613\n",
      "\n",
      "Tweet text: <otwarta ksika><ksiki>czyta trzeba, ale przed <kamera filmowa>kamerami nie zawsze jest to atwe<umiechnita twarz z otwartymi ustami>. zobaczcie sami<kciuk w gr>.\n",
      "\t True label: __label__neutral, predicted label: __label__ambiguous, probability: 0.1259327530860901\n",
      "\n",
      "Tweet text: nasz program nazywa si polska. dobrazmiana w nowej odsonie *niszy, b. proporcjonalny zus dla mniej zarabiajcych, *obnika podatku cit z 15 do 9 proc. dla maych firm, *due inwestycje w drogi gminne i powiatowe, *program czyste powietrze, ruszamy w polsk!\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.1778208613395691\n",
      "\n",
      "Tweet text: witam serdecznie! ledz pani bloga na salonie24! podziwiam od dawna! pozdrawima serdecznie!\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.5312193632125854\n",
      "\n",
      "Tweet text: udzielamwotum\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.003386611817404628\n",
      "\n",
      "Tweet text: prof. bartoszewski: wierz w rozsdek i patriotyzm polakw, w polsk, w mj nard. dobrze, e mamy takiego prezydenta popieramkomorowskiego\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.16027602553367615\n",
      "\n",
      "Tweet text: do dyktatury pisu, kleru, mczyzn i kobiet rownie takich jak pani premier, pose pawowicz, joanna banasik.\n",
      "\t True label: __label__negative, predicted label: __label__positive, probability: 0.5312193632125854\n",
      "\n",
      "Tweet text: do piotrkowic: w zeszym tygodniu przyjechaem do piotrkowic (gm. chmielnik) by razem ze straakami tamtejszej...\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.18714269995689392\n",
      "\n",
      "Tweet text: kady dzie niech zblia nas polakw do lepszej polski!\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.11597072333097458\n",
      "\n",
      "Tweet text: jednym z wikszych wyzwa dla gdyskiego portu jest budowa drugiego poczenia drogowego z obwodnic trjmiasta....\n",
      "\t True label: __label__neutral, predicted label: __label__positive, probability: 0.13661839067935944\n",
      "\n",
      "Tweet text: i wan uchwa o emisji obligacji, z ktrych sfinansujemy midzy innymi budow dwch szk: przy ul. sawinkowskiej i na felinie. okazao si take, e radni mieli duo pyta dotyczcych biecego funkcjonowania miasta. wszystkie inne sprawy odoylimy na inny termin sesji.\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.3140605390071869\n",
      "\n",
      "Tweet text: ogromny szacunek za prac, zaangaowanie i obdarowanie nas piknymi chwilami, podczas ktrych moglimy by dumni ze zwycistw polki. brawo!!!\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.3140605390071869\n",
      "\n",
      "Tweet text: wadza zamowia tylko 2,5 mln. szczepionek na gryp co wystarczy dla 6.7% populacji, take... aaa zamieni mieszkanie w gdyni w atrakcyjnej lokalizacji na 3 szczepionki na gryp. tylko powane oferty. marekdziaa\n",
      "\t True label: __label__negative, predicted label: __label__ambiguous, probability: 0.24509501457214355\n",
      "\n",
      "Tweet text: gratulacje dla legii - zrobili to co lech od trzech lat nie moe\n",
      "\t True label: __label__ambiguous, predicted label: __label__negative, probability: 0.2689514458179474\n",
      "\n",
      "Tweet text: oto skutki podsycania nienawici. finakampanii wypad2020\n",
      "\t True label: __label__negative, predicted label: __label__neutral, probability: 0.4378334879875183\n",
      "\n",
      "Tweet text: tarewolucja\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.004208795726299286\n",
      "\n",
      "Tweet text: <twarz z monoklem> dziwne... to miecicie si na jednej kanapie czy pufie?\n",
      "\t True label: __label__ambiguous, predicted label: __label__neutral, probability: 0.2509227991104126\n",
      "\n",
      "Tweet text: w ywcu witujemy 101 lat polskiej niepodlegoci. <flaga: polska> to niezwykle wane, abymy wci pamitali o najwaniejszych momentach dla naszego kraju i t pami przekazywali kolejnym pokoleniom, dbajc o nasz wolno kadego dnia. niech yje wolna i niepodlega \n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.287777841091156\n",
      "\n",
      "Tweet text: \"titanic\" tonie... co na to kapitan tusk?\n",
      "\t True label: __label__negative, predicted label: __label__ambiguous, probability: 0.11921291798353195\n",
      "\n",
      "Tweet text: debataprezydencka oby trzaskowski nie wali za bardzo pici w st, bo moe znowu co wypynie z czajki<twarz ze zami radoci>\n",
      "\t True label: __label__ambiguous, predicted label: __label__negative, probability: 0.4843900501728058\n",
      "\n",
      "Tweet text: mazowsze i lubelskie - oficjalne otwarcie mostu im. edwarda wojtasa. teraz czeka nas przebudowa drogi 747 do lipska i iy\n",
      "\t True label: __label__neutral, predicted label: __label__negative, probability: 0.287777841091156\n",
      "\n",
      "Tweet text: sprbuj z nim pogada i zapytam o co konkretnie chodzi.\n",
      "\t True label: __label__neutral, predicted label: __label__positive, probability: 0.12253321707248688\n",
      "\n",
      "Tweet text: ue jako kontynuacja europejskiej wsplnoty wgla i stali jest uni energetyczn bez specjalnych interwencji\n",
      "\t True label: __label__neutral, predicted label: __label__positive, probability: 0.30736804008483887\n",
      "\n",
      "Tweet text: t. piketty: potrzebujemy pastwa opiekuczego nie zamordystycznego lewica polska2020 wiat2020 polityka\n",
      "\t True label: __label__ambiguous, predicted label: __label__negative, probability: 0.6723417043685913\n",
      "\n",
      "Tweet text: pod pretekstem walki z mnieman pandemi totalniacka wadza w penym porozumieniu z tresowan opozycj proceduje wanie ustawowe ramy nowego apartheidu, segregacji - na razie dla niezamaskowanych, jutro dla nieszczepionych.\n",
      "\t True label: __label__negative, predicted label: __label__positive, probability: 0.22816647589206696\n",
      "\n",
      "Tweet text: nowa rzeczywisto caego wiata i europy to wyzwanie dla idei wsplnego dobra.polska pod prezydentur andrzeja dudy bdzie do brym tego wzorcem.\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.1824355274438858\n",
      "\n",
      "Tweet text: dotrzymujemy sowa i startujemy z modernizacj parku lskiego. w tym roku wydamy 30 milionw zotych. wyremontujemy .  hal \"kapelusz\"  kana regatowy  krgi taneczne  galeri rzeby lskiej...\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.59267657995224\n",
      "\n",
      "Tweet text: ktrym nie do koca si czuj. ale na piwo do krakowa zawsze zapraszam!\n",
      "\t True label: __label__positive, predicted label: __label__ambiguous, probability: 0.6584275364875793\n",
      "\n",
      "Tweet text: w 88 rocznic tragicznej mierci franciszka wirko i stanisawa wigury spotkalimy si pod pomnikiem lotnikw w bielsku-biaej <flaga: polska>. wraz z seniorami lotnictwa i modzie kadego roku czcimy pami bohaterw lotnictwa. wirkoiwigura posedrabek bielsko-biaa\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.40734341740608215\n",
      "\n",
      "Tweet text: po co tworzy partnerstwo dla wsprzdzenia europ jak mona powygraa od hitlerowcw i pomarzy o reparacjach?\n",
      "\t True label: __label__negative, predicted label: __label__positive, probability: 0.2568419873714447\n",
      "\n",
      "Tweet text: wczeniej rak krab huty stalowa wola podziwia prezydent rp mspo2016 wojsko duma\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.6297846436500549\n",
      "\n",
      "Tweet text: sowo ciaem si stao, bdziemy mieli nowego prezesa rady ministrw morawiecki. koniec spekulacji, strach w obozie wadzy, konsternacja w elektoracie pis, przykrywka ust. o sn i krs. ciekawe czasy...\n",
      "\t True label: __label__ambiguous, predicted label: __label__negative, probability: 0.22816647589206696\n",
      "\n",
      "Tweet text: trzaskowski2020\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.010338151827454567\n",
      "\n",
      "Tweet text: po dzisiejszej radzie ministrw wysaem zaproszenie do wszystkich przewodniczcych klubw parlamentarnych na spotkanie dotyczce propozycji zapisw ustawy chronicej dzieci i modzie przed pornografi w sieci. wierz, e wsplnie i ponad podziaami rozwiemy ten wany problem.\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.538993239402771\n",
      "\n",
      "Tweet text: dzisiejszy dzie zaczynamy msz wit i podzikowaniami dla marynarzy rzecznych, ktrzy 3 lipca obchodzili swoje wito. wrocaw\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.3140605390071869\n",
      "\n",
      "Tweet text: dzikuj wszystkim, ktrzy odwiedzili mojego bloga i dokonali wpisw na forum, a musz przyzna, e byo ich sporo\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.2509227991104126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_texts = list(test_tweets[\"text\"].values)\n",
    "test_labels = list(test_tweets[\"label\"].values)\n",
    "print(\"Tweets which were incorrrectly predicted:\")\n",
    "for i in range(len(test_texts)):\n",
    "    pred = model.predict([test_texts[i]])\n",
    "    if test_labels[i] != pred[0][0][0]:\n",
    "        print(f\"Tweet text: {test_texts[i]}\")\n",
    "        print(f\"\\t True label: {test_labels[i]}, predicted label: {pred[0][0][0]}, probability: {pred[1][0][0]}\")\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}