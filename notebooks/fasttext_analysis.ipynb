{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import fasttext\n",
    "import json\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PATH_TO_DATASETS = \"../datasets\"\n",
    "PATH_TO_POLEMO_CONLL = \"../datasets/polemo/dataset_conll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "files = {\n",
    "    \"train\": os.path.join(PATH_TO_POLEMO_CONLL,\"all.sentence.train_processed.csv\"),\n",
    "    \"dev\": os.path.join(PATH_TO_POLEMO_CONLL,\"all.sentence.dev_processed.csv\"),\n",
    "    \"test\": os.path.join(PATH_TO_POLEMO_CONLL,\"all.sentence.test_processed.csv\"),\n",
    "    \"annotation\": os.path.join(PATH_TO_DATASETS, \"sentiment_data\", \"political_tweets_annotations.csv\")\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "with open(os.path.join(\"..\", \"datasets\", \"emojis.json\"), encoding=\"utf-8\") as f:\n",
    "    emoji_mapping = json.load(f)\n",
    "\n",
    "emoji_mapping_items = emoji_mapping.items()\n",
    "def emoji2text_tweet(tweet: str) -> str:\n",
    "    text = tweet\n",
    "    for emoji, emoji_text in emoji_mapping_items:\n",
    "        text = text.replace(emoji, f\"<{emoji_text}>\")\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def remove_quotes_from_saved_file(txt_path: str):\n",
    "    text = \"\"\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line[0] == \"\\\"\" and line[-2] == \"\\\"\":\n",
    "                line = line[1:]\n",
    "                line = line[:-2] + \"\\n\"\n",
    "            text += line\n",
    "\n",
    "    os.remove(txt_path)\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "data_for_fasttext = {}\n",
    "for dataset, file_path in files.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[['label','text']]\n",
    "    df['label'] = \"__label__\" +  df['label']\n",
    "    df['text'] = df['text'].apply(emoji2text_tweet)\n",
    "    df['text'] = df['text'].apply(lambda string: string.lower())\n",
    "    df['text'] = df['text'].apply(lambda string: string.replace(\"#\",\"\"))\n",
    "    df['row'] = df['label'] + \" \" + df['text']\n",
    "    path = os.path.join(PATH_TO_DATASETS, \"sentiment_data\", f\"{dataset}_data.txt\")\n",
    "    df['row'].to_csv(path, index=False, header=False)\n",
    "    remove_quotes_from_saved_file(path)\n",
    "    data_for_fasttext[dataset] = {}\n",
    "    data_for_fasttext[dataset][\"labels\"] = list(df['label'].values)\n",
    "    data_for_fasttext[dataset][\"texts\"] = list(df['text'].values)\n",
    "    data_for_fasttext[dataset][\"dataframe\"] = df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "tweets_data = data_for_fasttext['annotation']\n",
    "texts_train_val, texts_test, labels_train_val, labels_test = train_test_split(tweets_data['texts'], tweets_data['labels'], test_size=0.1, random_state=42)\n",
    "texts_train, texts_val, labels_train, labels_val = train_test_split(texts_train_val, labels_train_val, test_size=1/9, random_state=42)\n",
    "\n",
    "train_polemo = data_for_fasttext['train']['dataframe'][[\"text\", \"label\"]]\n",
    "val_polemo = data_for_fasttext['dev']['dataframe'][[\"text\", \"label\"]]\n",
    "test_polemo = data_for_fasttext['test']['dataframe'][[\"text\", \"label\"]]\n",
    "\n",
    "train_tweets = pd.DataFrame(data={\"text\": texts_train, \"label\": labels_train})\n",
    "val_tweets = pd.DataFrame(data={\"text\": texts_val, \"label\": labels_val})\n",
    "test_tweets = pd.DataFrame(data={\"text\": texts_test, \"label\": labels_test})\n",
    "\n",
    "train = train_polemo.append(train_tweets)\n",
    "val = val_polemo.append(val_tweets)\n",
    "test = test_polemo.append(test_tweets)\n",
    "\n",
    "train['row'] = train['label'] + \" \" + train['text']\n",
    "val['row'] = val['label'] + \" \" + val['text']\n",
    "test['row'] = test['label'] + \" \" + test['text']\n",
    "\n",
    "train['row'].to_csv(os.path.join(PATH_TO_DATASETS, \"sentiment_data\", f\"full_train_data.txt\"), index=False, header=False)\n",
    "val['row'].to_csv(os.path.join(PATH_TO_DATASETS, \"sentiment_data\", f\"full_val_data.txt\"), index=False, header=False)\n",
    "test['row'].to_csv(os.path.join(PATH_TO_DATASETS, \"sentiment_data\", f\"full_test_data.txt\"), index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier only on PolEmo training data...\n",
      "F1-score for all tweets: 0.29466757589906445\n",
      "F1-score for test set of PolEmo: 0.5874823306117167\n",
      "\n",
      "Training classifier on PolEmo training data and 80% of political tweets\n",
      "F1-score for test set of political tweets: 0.3812850008377031\n",
      "F1-score for test set of PolEmo: 0.5610325791091721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training classifier only on PolEmo training data...\")\n",
    "model =fasttext.train_supervised(input=os.path.join(PATH_TO_DATASETS, \"sentiment_data\", \"train_data.txt\"), wordNgrams=1, neg=5,dim=300, lr=0.005, epoch=500, loss=\"ns\", verbose=1, label_prefix='__label__')\n",
    "test_results = model.predict(data_for_fasttext[\"test\"][\"texts\"])\n",
    "annotation_results = model.predict(data_for_fasttext[\"annotation\"][\"texts\"])\n",
    "\n",
    "print(f\"F1-score for all tweets: {f1_score(annotation_results[0],data_for_fasttext['annotation']['labels'],average='macro')}\")\n",
    "print(f\"F1-score for test set of PolEmo: {f1_score(test_results[0],data_for_fasttext['test']['labels'],average='macro')}\")\n",
    "print()\n",
    "\n",
    "print(\"Training classifier on PolEmo training data and 80% of political tweets\")\n",
    "model =fasttext.train_supervised(input=os.path.join(PATH_TO_DATASETS, \"sentiment_data\", \"full_train_data.txt\"), wordNgrams=1, neg=5,dim=300, lr=0.005, epoch=500, loss=\"ns\", verbose=1, label_prefix='__label__')\n",
    "test_results = model.predict(list(test_polemo[\"text\"].values))\n",
    "annotation_results = model.predict(list(test_tweets['text'].values))\n",
    "\n",
    "print(f\"F1-score for test set of political tweets: {f1_score(annotation_results[0],list(test_tweets['label'].values),average='macro')}\")\n",
    "print(f\"F1-score for test set of PolEmo: {f1_score(test_results[0],list(test_polemo['label'].values),average='macro')}\")\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5623942811063802\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5629298826329951\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.562571859518963\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.3500344840528849\n",
      "F1-score for dev set: 0.5629021398226923\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.36152677445247106\n",
      "F1-score for dev set: 0.5582780004991592\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.3601970461953892\n",
      "F1-score for dev set: 0.5592922691828756\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.367239299716516\n",
      "F1-score for dev set: 0.5564198703181898\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.367239299716516\n",
      "F1-score for dev set: 0.5560494890712804\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5706073642332963\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5704664458637649\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5700849443316581\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.57068949669332\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.41809287758654845\n",
      "F1-score for dev set: 0.5964858413727783\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.41809287758654845\n",
      "F1-score for dev set: 0.5961174314759348\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.41809287758654845\n",
      "F1-score for dev set: 0.5962649553694438\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.41809287758654845\n",
      "F1-score for dev set: 0.5965323319189448\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6031817829531319\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6022700759160577\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.601864468810706\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6015863097213002\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.605732347798267\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6054246922025561\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6056461452059755\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6041908521269723\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.39900082638419354\n",
      "F1-score for dev set: 0.5902343640042036\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.39900082638419354\n",
      "F1-score for dev set: 0.5899936012340775\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.39900082638419354\n",
      "F1-score for dev set: 0.5897528640697065\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.39900082638419354\n",
      "F1-score for dev set: 0.5898151012624974\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4330852287247636\n",
      "F1-score for dev set: 0.5964765228893903\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43242796360266245\n",
      "F1-score for dev set: 0.5963029705464398\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43242796360266245\n",
      "F1-score for dev set: 0.5950023738027166\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43242796360266245\n",
      "F1-score for dev set: 0.5942141939826172\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.45773524720893144\n",
      "F1-score for dev set: 0.595838509699971\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.45685341299376386\n",
      "F1-score for dev set: 0.5959876525220141\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.45685341299376386\n",
      "F1-score for dev set: 0.5963027592912202\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.45685341299376386\n",
      "F1-score for dev set: 0.5961264799049685\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4287878787878788\n",
      "F1-score for dev set: 0.578241469359867\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.4287878787878788\n",
      "F1-score for dev set: 0.5780949693429496\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.4287878787878788\n",
      "F1-score for dev set: 0.5780318706504539\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.4287878787878788\n",
      "F1-score for dev set: 0.5781989423181655\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.58565814271643\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.5846522159345713\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.5849852841231988\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.5859802479593381\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5883855813828512\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5885487698767945\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5886762355254658\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 4\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5886762355254658\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5717001098326694\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.572027866196775\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5718699674227063\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5718699674227063\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4989115474077759\n",
      "F1-score for dev set: 0.578612027993442\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.4885352339814949\n",
      "F1-score for dev set: 0.5793203197874702\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.4627725035161744\n",
      "F1-score for dev set: 0.5767334745329324\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.4885352339814949\n",
      "F1-score for dev set: 0.5750089062371991\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.44657598867835524\n",
      "F1-score for dev set: 0.5799423032404591\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.44657598867835524\n",
      "F1-score for dev set: 0.5807364590481098\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.44657598867835524\n",
      "F1-score for dev set: 0.5807364590481098\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 300\n",
      "Ngram - 5\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.44657598867835524\n",
      "F1-score for dev set: 0.5807364590481098\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5625205958490247\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5617300653618835\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5626973463916634\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5623865660464368\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.36612220873339957\n",
      "F1-score for dev set: 0.5564832202428006\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.36612220873339957\n",
      "F1-score for dev set: 0.5578504995255673\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.367239299716516\n",
      "F1-score for dev set: 0.5563937762031202\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.3611425339366516\n",
      "F1-score for dev set: 0.5551823336884754\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5705791619755132\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5694953991109653\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5708618017026222\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5695170101135963\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5960740521596544\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5960522855118234\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5958891375307778\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5959014017503925\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6024626999103988\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6024241284092978\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6008479437826201\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6006420705828094\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6038631575992107\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6047068496903283\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6043803336380296\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6043846940144177\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.5876384828014878\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.5887840972994121\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.5881732757452676\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.5880546767550845\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4330852287247636\n",
      "F1-score for dev set: 0.5962680358700545\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43242796360266245\n",
      "F1-score for dev set: 0.5968775810451735\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43242796360266245\n",
      "F1-score for dev set: 0.5951143331641728\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43242796360266245\n",
      "F1-score for dev set: 0.5943461994806127\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.45773524720893144\n",
      "F1-score for dev set: 0.5964579763289016\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.45773524720893144\n",
      "F1-score for dev set: 0.5961063100557479\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.45773524720893144\n",
      "F1-score for dev set: 0.596329579869589\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.45773524720893144\n",
      "F1-score for dev set: 0.5963056140151539\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.42163633467981293\n",
      "F1-score for dev set: 0.5767430521840395\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.42163633467981293\n",
      "F1-score for dev set: 0.5767313243560706\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.42163633467981293\n",
      "F1-score for dev set: 0.5765753511024818\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.42163633467981293\n",
      "F1-score for dev set: 0.5765931618705866\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.5855133146722856\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.5845263915117236\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.5851894549453599\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43316610925306576\n",
      "F1-score for dev set: 0.585169571399017\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5878358317484796\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5871059184116542\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.587897041775314\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 4\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.45752379550583844\n",
      "F1-score for dev set: 0.5881741909456453\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5698290272947197\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5696321884357409\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5698290272947197\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.40264388921677924\n",
      "F1-score for dev set: 0.5698033238253848\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4989115474077759\n",
      "F1-score for dev set: 0.5781318888545635\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.4885352339814949\n",
      "F1-score for dev set: 0.580648497585905\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.4885352339814949\n",
      "F1-score for dev set: 0.5782755314640016\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.4885352339814949\n",
      "F1-score for dev set: 0.5773602750116107\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4393732492997199\n",
      "F1-score for dev set: 0.5788925140297055\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.4393732492997199\n",
      "F1-score for dev set: 0.5795021232622986\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.44657598867835524\n",
      "F1-score for dev set: 0.5786816349500974\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 500\n",
      "Ngram - 5\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.44657598867835524\n",
      "F1-score for dev set: 0.5796255100354513\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.34984710896960713\n",
      "F1-score for dev set: 0.5632997887206401\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.3500344840528849\n",
      "F1-score for dev set: 0.5627443738750487\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.3500344840528849\n",
      "F1-score for dev set: 0.5634742391696247\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.3500344840528849\n",
      "F1-score for dev set: 0.5629397978184261\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.36612220873339957\n",
      "F1-score for dev set: 0.557038219265292\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.3600246477577898\n",
      "F1-score for dev set: 0.5581052282131655\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.37326506587625674\n",
      "F1-score for dev set: 0.5568494278760834\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.37326506587625674\n",
      "F1-score for dev set: 0.5552685793608568\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5701158764554315\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5694119001074792\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5693101927126345\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 1\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.38135152696556207\n",
      "F1-score for dev set: 0.5700904054979502\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5946319975901299\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5943110117810346\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5948872064014958\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.3986903083248243\n",
      "F1-score for dev set: 0.5943293136436751\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6026093285004853\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6018275226901302\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6013123296488856\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.43428995485045774\n",
      "F1-score for dev set: 0.6007268472771755\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6043218635195131\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6044032434171132\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6042658336779875\n",
      "\n",
      "Loss method - softmax\n",
      "Dim - 1000\n",
      "Ngram - 2\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.41696254486785334\n",
      "F1-score for dev set: 0.6048047717004964\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.5880047168330594\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 3\n",
      "Negative samples - 10\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.588038206554259\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 3\n",
      "Negative samples - 15\n",
      "F1-score for all tweets: 0.41569928137092316\n",
      "F1-score for dev set: 0.5881203378409153\n",
      "\n",
      "Loss method - hs\n",
      "Dim - 1000\n",
      "Ngram - 3\n",
      "Negative samples - 20\n",
      "F1-score for all tweets: 0.39900082638419354\n",
      "F1-score for dev set: 0.5876451344999448\n",
      "\n",
      "Loss method - ns\n",
      "Dim - 1000\n",
      "Ngram - 3\n",
      "Negative samples - 5\n",
      "F1-score for all tweets: 0.4330852287247636\n",
      "F1-score for dev set: 0.5960209072677074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dims = []\n",
    "ngrams = []\n",
    "losses = []\n",
    "negs = []\n",
    "tweets_f1_scores = []\n",
    "dev_f1_scores = []\n",
    "for dim in [300, 500]:\n",
    "    for ngram in [1,2,3,4,5]:\n",
    "        for method in [\"hs\", \"ns\", \"softmax\"]:\n",
    "            for neg in [5,10,15,20]:\n",
    "                model =fasttext.train_supervised(input=os.path.join(PATH_TO_DATASETS, \"sentiment_data\", \"full_train_data.txt\"), wordNgrams=ngram, neg=neg,dim=dim, lr=0.005, epoch=500, loss=method, verbose=1, label_prefix='__label__')\n",
    "                dev_results = model.predict(list(val_polemo[\"text\"].values))\n",
    "                annotation_results = model.predict(list(val_tweets[\"text\"].values))\n",
    "                dims.append(dim)\n",
    "                ngrams.append(ngram)\n",
    "                losses.append(method)\n",
    "                negs.append(neg)\n",
    "                tweets_f1_scores.append(f1_score(annotation_results[0],list(val_tweets['label'].values),average='macro'))\n",
    "                dev_f1_scores.append(f1_score(dev_results[0],list(val_polemo['label'].values),average='macro'))\n",
    "                print(f\"Loss method - {method}\")\n",
    "                print(f\"Dim - {dim}\")\n",
    "                print(f\"Ngram - {ngram}\")\n",
    "                print(f\"Negative samples - {neg}\")\n",
    "                print(f\"F1-score for all tweets: {tweets_f1_scores[-1]}\")\n",
    "                print(f\"F1-score for dev set: {dev_f1_scores[-1]}\")\n",
    "                print()\n",
    "\n",
    "results = pd.DataFrame(data={\"dim\": dims,\n",
    "                             \"ngram\" : ngrams,\n",
    "                             \"loss\": losses,\n",
    "                             \"neg\": negs,\n",
    "                             \"tweets_f1_score\": tweets_f1_scores,\n",
    "                             \"dev_f1_score\": dev_f1_scores})\n",
    "\n",
    "results.to_csv(os.path.join(\"..\",\"reports\",\"sentiment_classification_results.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate - 0.001\n",
      "Epochs - 100\n",
      "F1-score for all tweets: 0.07723577235772357\n",
      "F1-score for dev set: 0.13483288855000636\n",
      "\n",
      "Learning rate - 0.001\n",
      "Epochs - 250\n",
      "F1-score for all tweets: 0.28369486793399834\n",
      "F1-score for dev set: 0.28733619777780717\n",
      "\n",
      "Learning rate - 0.001\n",
      "Epochs - 500\n",
      "F1-score for all tweets: 0.3450963718820861\n",
      "F1-score for dev set: 0.4875128174787702\n",
      "\n",
      "Learning rate - 0.001\n",
      "Epochs - 1000\n",
      "F1-score for all tweets: 0.44096459096459095\n",
      "F1-score for dev set: 0.5607791230695027\n",
      "\n",
      "Learning rate - 0.005\n",
      "Epochs - 100\n",
      "F1-score for all tweets: 0.3450963718820861\n",
      "F1-score for dev set: 0.4872309231673328\n",
      "\n",
      "Learning rate - 0.005\n",
      "Epochs - 250\n",
      "F1-score for all tweets: 0.4589617898441427\n",
      "F1-score for dev set: 0.5740606491264695\n",
      "\n",
      "Learning rate - 0.005\n",
      "Epochs - 500\n",
      "F1-score for all tweets: 0.4989115474077759\n",
      "F1-score for dev set: 0.5780589677968582\n",
      "\n",
      "Learning rate - 0.005\n",
      "Epochs - 1000\n",
      "F1-score for all tweets: 0.4441432764630343\n",
      "F1-score for dev set: 0.5730638765390956\n",
      "\n",
      "Learning rate - 0.0001\n",
      "Epochs - 100\n",
      "F1-score for all tweets: 0.08022774327122154\n",
      "F1-score for dev set: 0.12492594192404008\n",
      "\n",
      "Learning rate - 0.0001\n",
      "Epochs - 250\n",
      "F1-score for all tweets: 0.11953551912568307\n",
      "F1-score for dev set: 0.1347864768683274\n",
      "\n",
      "Learning rate - 0.0001\n",
      "Epochs - 500\n",
      "F1-score for all tweets: 0.07723577235772357\n",
      "F1-score for dev set: 0.13483288855000636\n",
      "\n",
      "Learning rate - 0.0001\n",
      "Epochs - 1000\n",
      "F1-score for all tweets: 0.07377049180327869\n",
      "F1-score for dev set: 0.13483288855000636\n",
      "\n",
      "Learning rate - 0.0005\n",
      "Epochs - 100\n",
      "F1-score for all tweets: 0.07723577235772357\n",
      "F1-score for dev set: 0.13483288855000636\n",
      "\n",
      "Learning rate - 0.0005\n",
      "Epochs - 250\n",
      "F1-score for all tweets: 0.07377049180327869\n",
      "F1-score for dev set: 0.135181151247235\n",
      "\n",
      "Learning rate - 0.0005\n",
      "Epochs - 500\n",
      "F1-score for all tweets: 0.2913510101010101\n",
      "F1-score for dev set: 0.2865765515902747\n",
      "\n",
      "Learning rate - 0.0005\n",
      "Epochs - 1000\n",
      "F1-score for all tweets: 0.3450963718820861\n",
      "F1-score for dev set: 0.48687333663868504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrs = []\n",
    "epochs = []\n",
    "tweets_f1_scores = []\n",
    "dev_f1_scores = []\n",
    "for lr in [0.001, 0.005, 0.0001, 0.0005]:\n",
    "    for epoch in [100,250,500,1000]:\n",
    "        model =fasttext.train_supervised(input=os.path.join(PATH_TO_DATASETS, \"sentiment_data\", \"full_train_data.txt\"), wordNgrams=5, neg=5,dim=300, lr=lr, epoch=epoch, loss=\"ns\", verbose=1, label_prefix='__label__')\n",
    "        dev_results = model.predict(list(val_polemo[\"text\"].values))\n",
    "        annotation_results = model.predict(list(val_tweets[\"text\"].values))\n",
    "        lrs.append(lr)\n",
    "        epochs.append(epoch)\n",
    "        tweets_f1_scores.append(f1_score(annotation_results[0],list(val_tweets['label'].values),average='macro'))\n",
    "        dev_f1_scores.append(f1_score(dev_results[0],list(val_polemo['label'].values),average='macro'))\n",
    "        print(f\"Learning rate - {lr}\")\n",
    "        print(f\"Epochs - {epoch}\")\n",
    "        print(f\"F1-score for all tweets: {tweets_f1_scores[-1]}\")\n",
    "        print(f\"F1-score for dev set: {dev_f1_scores[-1]}\")\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "lr_epoch_results = pd.DataFrame(data={\"lr\": lrs,\n",
    "                             \"epoch\" : epochs,\n",
    "                             \"tweets_f1_score\": tweets_f1_scores,\n",
    "                             \"dev_f1_score\": dev_f1_scores})\n",
    "\n",
    "lr_epoch_results.to_csv(os.path.join(\"..\",\"reports\",\"sentiment_classification_lr_epoch_results.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=os.path.join(PATH_TO_DATASETS, \"sentiment_data\", \"full_train_data.txt\"), wordNgrams=5, neg=5,dim=300, lr=0.005, epoch=500, loss=\"ns\", verbose=1, label_prefix='__label__')\n",
    "test_results = model.predict(list(test_polemo[\"text\"].values))\n",
    "annotation_results = model.predict(list(test_tweets[\"text\"].values))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for all tweets: 0.42461047925843143\n",
      "F1-score for dev set: 0.5924934125716439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1-score for all tweets: {f1_score(annotation_results[0],list(test_tweets['label'].values),average='macro')}\")\n",
    "print(f\"F1-score for dev set: {f1_score(test_results[0],list(test_polemo['label'].values),average='macro')}\")\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for political tweets\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "__label__ambiguous       0.00      0.00      0.00         5\n",
      " __label__negative       0.52      0.62      0.57        24\n",
      "  __label__neutral       0.49      0.69      0.57        26\n",
      " __label__positive       0.70      0.47      0.56        49\n",
      "\n",
      "          accuracy                           0.54       104\n",
      "         macro avg       0.43      0.45      0.42       104\n",
      "      weighted avg       0.57      0.54      0.54       104\n",
      "\n",
      "Classification report for polemo data\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "__label__ambiguous       0.48      0.23      0.31       681\n",
      " __label__negative       0.63      0.84      0.72      2123\n",
      "  __label__neutral       0.69      0.61      0.65      1419\n",
      " __label__positive       0.75      0.66      0.70      1522\n",
      "\n",
      "          accuracy                           0.66      5745\n",
      "         macro avg       0.64      0.58      0.59      5745\n",
      "      weighted avg       0.66      0.66      0.65      5745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report for political tweets\")\n",
    "print(classification_report(list(test_tweets['label'].values), annotation_results[0]))\n",
    "\n",
    "print(\"Classification report for polemo data\")\n",
    "print(classification_report(list(test_polemo['label'].values), test_results[0]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets which were incorrrectly predicted:\n",
      "Tweet text: dzisiaj spotkanie śzgip z parlamentarzystami z woj. śl. dobra frekwencja. nikt z posłów nie bronił krytykowanych przez samorząd projektów.\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.14805719256401062\n",
      "\n",
      "Tweet text: ✔️projekt gotowy! <dłoń z palcem wskazującym w prawo> dotrzymuję słowa. 18 maja 2019 r. podczas spotkania z emerytowanymi górnikami w wałbrzychu obiecałam przygotować nowelizację ustawy z dnia 17 grudnia 1998 r. o emeryturach i rentach z funduszu ubezpieczeń społecznych. sejmrp nowelizacja emeryci górnicy\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.44553956389427185\n",
      "\n",
      "Tweet text: premier jest już w pszczyna dotrzymujemysłowa\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.11597072333097458\n",
      "\n",
      "Tweet text: to jest skandal!\n",
      "\t True label: __label__negative, predicted label: __label__ambiguous, probability: 0.0015587612288072705\n",
      "\n",
      "Tweet text: - nie zejdziemy ze swej drogi, dalej będziemy reformować wymiar sprawiedliwości, bo tego potrzebuje rzeczpospolita i jej obywatele, nie ugniemy się pod naciskami ani wewnętrznymi, ani zewnętrznymi – napisał j.kaczyński do uczestników konwencji sprawiedliwapolska\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.6859594583511353\n",
      "\n",
      "Tweet text: po pierwsze, jestem na posiedzeniu komisji ustawodawczej, która pracuje nad ustawą o komisji ds. pedofilii. po drugie, nie mogę uczestniczyć w pracach organu, którego większość składu została wybrana z naruszeniem konstytucji.\n",
      "\t True label: __label__neutral, predicted label: __label__negative, probability: 0.3775506913661957\n",
      "\n",
      "Tweet text: dziś wybory do izb rolniczych idźcie głosować <przycisk z krzyżykiem> to ważne\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.3276783227920532\n",
      "\n",
      "Tweet text: prezydent : ja nie mam żadnego problemu z tym, że czy są bardzo majętnymi ludźmi. ja mam problem z tym, że przepisują majtki na żony i probują to ukryć. arenaprezydencka trzaskowski2020\n",
      "\t True label: __label__negative, predicted label: __label__positive, probability: 0.13297423720359802\n",
      "\n",
      "Tweet text: kocham <serce>️\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.1968362182378769\n",
      "\n",
      "Tweet text: wyruszamy z twojsztab na spotkania z mieszkańcami mazowsza i woj. podlaskiego porozmawiajmy\n",
      "\t True label: __label__neutral, predicted label: __label__positive, probability: 0.22271016240119934\n",
      "\n",
      "Tweet text: z pozdrowieniami dla pewnego księdza...\n",
      "\t True label: __label__negative, predicted label: __label__positive, probability: 0.14805719256401062\n",
      "\n",
      "Tweet text: grzegorz braun: naprzód polsko! zarządzajmy narodową werwą w miejsce obecnego zarządzania strachem! - youtube\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.4843900501728058\n",
      "\n",
      "Tweet text: prezes jarosław kaczyński , rada polityczna prawa i sprawiedliwości przez aklamację poparła kandydaturę andrzeja dudy na prezydenta rp.\n",
      "\t True label: __label__neutral, predicted label: __label__positive, probability: 0.29422497749328613\n",
      "\n",
      "Tweet text: <otwarta książka><książki>czytać trzeba, ale przed <kamera filmowa>kamerami nie zawsze jest to łatwe<uśmiechnięta twarz z otwartymi ustami>. zobaczcie sami<kciuk w górę>.\n",
      "\t True label: __label__neutral, predicted label: __label__ambiguous, probability: 0.1259327530860901\n",
      "\n",
      "Tweet text: nasz program nazywa się polska. dobrazmiana w nowej odsłonie *niższy, b. proporcjonalny zus dla mniej zarabiających, *obniżka podatku cit z 15 do 9 proc. dla małych firm, *duże inwestycje w drogi gminne i powiatowe, *program czyste powietrze, ruszamy w polskę!\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.1778208613395691\n",
      "\n",
      "Tweet text: witam serdecznie! śledzę pani bloga na salonie24! podziwiam od dawna! pozdrawima serdecznie!\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.5312193632125854\n",
      "\n",
      "Tweet text: udzielamwotum\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.003386611817404628\n",
      "\n",
      "Tweet text: prof. bartoszewski: wierzę w rozsądek i patriotyzm polaków, w polskę, w mój naród. dobrze, że mamy takiego prezydenta popieramkomorowskiego\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.16027602553367615\n",
      "\n",
      "Tweet text: dość dyktatury pisu, kleru, mężczyzn i kobiet rownież takich jak pani premier, poseł pawłowicz, joanna banasik.\n",
      "\t True label: __label__negative, predicted label: __label__positive, probability: 0.5312193632125854\n",
      "\n",
      "Tweet text: do piotrkowic…: w zeszłym tygodniu przyjechałem do piotrkowic (gm. chmielnik) by razem ze strażakami tamtejszej...\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.18714269995689392\n",
      "\n",
      "Tweet text: każdy dzień niech zbliża nas polaków do lepszej polski!\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.11597072333097458\n",
      "\n",
      "Tweet text: jednym z większych wyzwań dla gdyńskiego portu jest budowa drugiego połączenia drogowego z obwodnicą trójmiasta....\n",
      "\t True label: __label__neutral, predicted label: __label__positive, probability: 0.13661839067935944\n",
      "\n",
      "Tweet text: i ważną uchwałę o emisji obligacji, z których sfinansujemy między innymi budowę dwóch szkół: przy ul. sławinkowskiej i na felinie. okazało się także, że radni mieli dużo pytań dotyczących bieżącego funkcjonowania miasta. wszystkie inne sprawy odłożyliśmy na inny termin sesji.\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.3140605390071869\n",
      "\n",
      "Tweet text: ogromny szacunek za pracę, zaangażowanie i obdarowanie nas pięknymi chwilami, podczas których mogliśmy być dumni ze zwycięstw polki. brawo!!!\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.3140605390071869\n",
      "\n",
      "Tweet text: władza zamowiła tylko 2,5 mln. szczepionek na grypę co wystarczy dla 6.7% populacji, także... aaa zamienię mieszkanie w gdyni w atrakcyjnej lokalizacji na 3 szczepionki na grypę. tylko poważne oferty. marekdziała\n",
      "\t True label: __label__negative, predicted label: __label__ambiguous, probability: 0.24509501457214355\n",
      "\n",
      "Tweet text: gratulacje dla legii - zrobili to co lech od trzech lat nie może\n",
      "\t True label: __label__ambiguous, predicted label: __label__negative, probability: 0.2689514458179474\n",
      "\n",
      "Tweet text: oto skutki podsycania nienawiści. finałkampanii wypad2020\n",
      "\t True label: __label__negative, predicted label: __label__neutral, probability: 0.4378334879875183\n",
      "\n",
      "Tweet text: żółtarewolucja\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.004208795726299286\n",
      "\n",
      "Tweet text: <twarz z monoklem> dziwne... to mieścicie się na jednej kanapie czy pufie?\n",
      "\t True label: __label__ambiguous, predicted label: __label__neutral, probability: 0.2509227991104126\n",
      "\n",
      "Tweet text: w żywcu świętujemy 101 lat polskiej niepodległości. <flaga: polska>️ to niezwykle ważne, abyśmy wciąż pamiętali o najważniejszych momentach dla naszego kraju i tę pamięć przekazywali kolejnym pokoleniom, dbając o naszą wolność każdego dnia. niech żyje wolna i niepodległa ‼️\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.287777841091156\n",
      "\n",
      "Tweet text: \"titanic\" tonie... co na to kapitan tusk?\n",
      "\t True label: __label__negative, predicted label: __label__ambiguous, probability: 0.11921291798353195\n",
      "\n",
      "Tweet text: debataprezydencka oby trzaskowski nie walił za bardzo pięścią w stół, bo może znowu coś wypłynie z „czajki”<twarz ze łzami radości>\n",
      "\t True label: __label__ambiguous, predicted label: __label__negative, probability: 0.4843900501728058\n",
      "\n",
      "Tweet text: mazowsze i lubelskie - oficjalne otwarcie mostu im. edwarda wojtasa. teraz czeka nas przebudowa drogi 747 do lipska i iłży\n",
      "\t True label: __label__neutral, predicted label: __label__negative, probability: 0.287777841091156\n",
      "\n",
      "Tweet text: spróbuję z nim pogadać i zapytam o co konkretnie chodzi.\n",
      "\t True label: __label__neutral, predicted label: __label__positive, probability: 0.12253321707248688\n",
      "\n",
      "Tweet text: ue jako kontynuacja europejskiej wspólnoty węgla i stali jest unią energetyczną bez specjalnych interwencji\n",
      "\t True label: __label__neutral, predicted label: __label__positive, probability: 0.30736804008483887\n",
      "\n",
      "Tweet text: t. piketty: potrzebujemy państwa opiekuńczego nie zamordystycznego lewica polska2020 świat2020 polityka\n",
      "\t True label: __label__ambiguous, predicted label: __label__negative, probability: 0.6723417043685913\n",
      "\n",
      "Tweet text: pod pretekstem walki z mniemaną pandemią totalniacka władza w pełnym porozumieniu z tresowaną opozycją proceduje właśnie ustawowe ramy nowego apartheidu, segregacji - na razie dla niezamaskowanych, jutro dla nieszczepionych.\n",
      "\t True label: __label__negative, predicted label: __label__positive, probability: 0.22816647589206696\n",
      "\n",
      "Tweet text: nowa rzeczywistość całego świata i europy to wyzwanie dla idei wspólnego dobra.polska pod prezydenturą andrzeja dudy będzie do brym tego wzorcem.\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.1824355274438858\n",
      "\n",
      "Tweet text: dotrzymujemy słowa i startujemy z modernizacją parku śląskiego. w tym roku wydamy 3️⃣️0️⃣️ milionów złotych. wyremontujemy . ➡️ halę \"kapelusz\" ➡️ kanał regatowy ➡️ kręgi taneczne ➡️ galerię rzeźby śląskiej...\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.59267657995224\n",
      "\n",
      "Tweet text: którym nie do końca się czuję. ale na piwo do krakowa zawsze zapraszam!\n",
      "\t True label: __label__positive, predicted label: __label__ambiguous, probability: 0.6584275364875793\n",
      "\n",
      "Tweet text: w 88 rocznicę tragicznej śmierci franciszka żwirko i stanisława wigury spotkaliśmy się pod pomnikiem lotników w bielsku-białej <flaga: polska>. wraz z seniorami lotnictwa i młodzieżą każdego roku czcimy pamięć bohaterów lotnictwa. żwirkoiwigura posełdrabek bielsko-biała\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.40734341740608215\n",
      "\n",
      "Tweet text: po co tworzyć partnerstwo dla współrządzenia europą jak można powygrażać od hitlerowców i pomarzyć o reparacjach?\n",
      "\t True label: __label__negative, predicted label: __label__positive, probability: 0.2568419873714447\n",
      "\n",
      "Tweet text: wcześniej rak krab huty stalowa wola podziwiał prezydent rp mspo2016 wojsko duma\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.6297846436500549\n",
      "\n",
      "Tweet text: słowo ciałem się stało, będziemy mieli nowego prezesa rady ministrów morawiecki. koniec spekulacji, strach w obozie władzy, konsternacja w elektoracie pis, przykrywka ust. o sn i krs. ciekawe czasy...\n",
      "\t True label: __label__ambiguous, predicted label: __label__negative, probability: 0.22816647589206696\n",
      "\n",
      "Tweet text: trzaskowski2020\n",
      "\t True label: __label__positive, predicted label: __label__neutral, probability: 0.010338151827454567\n",
      "\n",
      "Tweet text: po dzisiejszej radzie ministrów wysłałem zaproszenie do wszystkich przewodniczących klubów parlamentarnych na spotkanie dotyczące propozycji zapisów ustawy chroniącej dzieci i młodzież przed pornografią w sieci. wierzę, że wspólnie i ponad podziałami rozwiążemy ten ważny problem.\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.538993239402771\n",
      "\n",
      "Tweet text: dzisiejszy dzień zaczynamy mszą świętą i podziękowaniami dla marynarzy rzecznych, którzy 3 lipca obchodzili swoje święto. wrocław\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.3140605390071869\n",
      "\n",
      "Tweet text: dziękuję wszystkim, którzy odwiedzili mojego bloga i dokonali wpisów na forum, a muszę przyznać, że było ich sporo\n",
      "\t True label: __label__positive, predicted label: __label__negative, probability: 0.2509227991104126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_texts = list(test_tweets[\"text\"].values)\n",
    "test_labels = list(test_tweets[\"label\"].values)\n",
    "print(\"Tweets which were incorrrectly predicted:\")\n",
    "for i in range(len(test_texts)):\n",
    "    pred = model.predict([test_texts[i]])\n",
    "    if test_labels[i] != pred[0][0][0]:\n",
    "        print(f\"Tweet text: {test_texts[i]}\")\n",
    "        print(f\"\\t True label: {test_labels[i]}, predicted label: {pred[0][0][0]}, probability: {pred[1][0][0]}\")\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}